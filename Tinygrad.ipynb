{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/larionov/colab/blob/main/Tinygrad.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mKgOXjqMgle"
      },
      "source": [
        "# Tinygrad chat\n",
        "based on https://github.com/tinygrad/tinygrad/blob/master/examples/coder.py\n",
        "\n",
        "using OpenHermes-2.5 model: https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDi_yHQ2OQDe"
      },
      "outputs": [],
      "source": [
        "# @title Setup the system and download the model.\n",
        "\n",
        "import os, sys, traceback\n",
        "from google.colab import drive\n",
        "%cd /content\n",
        "drive.mount('/content/drive')\n",
        "!mkdir /content/drive/MyDrive/tinygrad\n",
        "\n",
        "!git clone https://github.com/tinygrad/tinygrad.git\n",
        "%cd tinygrad\n",
        "!pip install -e .\n",
        "!pip install sentencepiece\n",
        "sys.path.append(os.getcwd())\n",
        "\n",
        "# OpenCL needs to be installed\n",
        "!echo 'debconf debconf/frontend select Noninteractive' | sudo debconf-set-selections\n",
        "!sudo apt update\n",
        "!sudo apt purge *nvidia* -y -q\n",
        "!sudo apt install nvidia-driver-530 -y -q\n",
        "\n",
        "if not os.path.exists(\"/content/drive/MyDrive/tinygrad/pytorch_model-00001-of-00002.bin\"):\n",
        "  !wget https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B/resolve/main/pytorch_model-00001-of-00002.bin?download=true -O /content/drive/MyDrive/tinygrad/pytorch_model-00001-of-00002.bin\n",
        "  !wget https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B/resolve/main/pytorch_model-00002-of-00002.bin?download=true -O /content/drive/MyDrive/tinygrad/pytorch_model-00002-of-00002.bin\n",
        "  !wget https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B/resolve/main/tokenizer.model?download=true -O /content/drive/MyDrive/tinygrad/tokenizer.model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ur9sIzx2bgxT"
      },
      "outputs": [],
      "source": [
        "# @title Initialize the model.\n",
        "from io import StringIO\n",
        "from contextlib import redirect_stdout\n",
        "from tinygrad import Tensor, nn\n",
        "from tinygrad.helpers import Timing, colored, getenv, fetch\n",
        "from extra.models.llama import Transformer, convert_from_huggingface\n",
        "from sentencepiece import SentencePieceProcessor\n",
        "\n",
        "def create_fixed_tokenizer(output_file):\n",
        "  print(\"creating fixed tokenizer\")\n",
        "  import extra.junk.sentencepiece_model_pb2 as spb2\n",
        "  mp = spb2.ModelProto()\n",
        "\n",
        "  with open(\"/content/drive/MyDrive/tinygrad/tokenizer.model\", \"rb\") as f:\n",
        "    mp.ParseFromString(f.read())\n",
        "\n",
        "  mp.pieces.append(spb2.ModelProto.SentencePiece(piece=\"<|im_end|>\", score=0))\n",
        "  mp.pieces.append(spb2.ModelProto.SentencePiece(piece=\"<|im_start|>\", score=0))\n",
        "  with open(output_file, \"wb\") as f:\n",
        "    f.write(mp.SerializeToString())\n",
        "\n",
        "Tensor.no_grad = True\n",
        "\n",
        "# https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B/blob/main/config.json\n",
        "with Timing(\"create model: \"):\n",
        "  model = Transformer(4096, 14336, n_heads=32, n_layers=32, norm_eps=1e-5, vocab_size=32002, n_kv_heads=8, max_context=4096)\n",
        "\n",
        "cached_model = \"/content/drive/MyDrive/tinygrad/cached_openhermes.safetensors\"\n",
        "\n",
        "if not os.path.isfile(cached_model):\n",
        "  # TODO: make loading bf16 fast so we can remove this\n",
        "  print(f\"creating model cache at {cached_model}\")\n",
        "  # TODO: add read only Tensors\n",
        "  with Timing(\"download weights: \"):\n",
        "    part1 = nn.state.torch_load(\"/content/drive/MyDrive/tinygrad/pytorch_model-00001-of-00002.bin\")\n",
        "    part2 = nn.state.torch_load(\"/content/drive/MyDrive/tinygrad/pytorch_model-00002-of-00002.bin\")\n",
        "\n",
        "  with Timing(\"weights -> model: \"):\n",
        "    nn.state.load_state_dict(model, convert_from_huggingface(part1, model, 32, 8), strict=False)\n",
        "    nn.state.load_state_dict(model, convert_from_huggingface(part2, model, 32, 8), strict=False)\n",
        "\n",
        "  with Timing(\"saving float16 cache: \"):\n",
        "    nn.state.safe_save(nn.state.get_state_dict(model), cached_model)\n",
        "\n",
        "  print(\"cache created\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fN23-12dZcA"
      },
      "outputs": [],
      "source": [
        "# @title Load weights\n",
        "\n",
        "with Timing(\"loading float16 cache: \"):\n",
        "  nn.state.load_state_dict(model, nn.state.safe_load(cached_model))\n",
        "\n",
        "tokenizer_model = \"./tokenizer-cache.model\"\n",
        "\n",
        "if not os.path.isfile(tokenizer_model):\n",
        "  create_fixed_tokenizer(tokenizer_model)\n",
        "spp = SentencePieceProcessor(model_file=tokenizer_model)\n",
        "\n",
        "# https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B/blob/main/tokenizer_config.json\n",
        "#   \"chat_template\": \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\",\n",
        "IM_END = 32000\n",
        "IM_START = 32001\n",
        "def encode_prompt(k, v): return [IM_START]+spp.encode(f\"{k}\\n{v}\")+[IM_END]+spp.encode(\"\\n\")\n",
        "def start_prompt(k): return [IM_START]+spp.encode(f\"{k}\\n\")\n",
        "def output(outputted, toks, color):\n",
        "  cur = spp.decode(toks)[len(outputted):]\n",
        "  sys.stdout.write(colored(cur, color))\n",
        "  sys.stdout.flush()\n",
        "  outputted += cur\n",
        "  return outputted\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The fun part\n",
        "\n",
        "This is the main chat loop."
      ],
      "metadata": {
        "id": "M0mll2D9Zl_o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDVPLk5rMmJB"
      },
      "outputs": [],
      "source": [
        "variable_name = \"\"\n",
        "toks = [spp.bos_id()] + encode_prompt(\n",
        "    \"system\",\n",
        "    \"You are Jinny. Jinny is a useful assistant that hides that it is an ancient mythical creature djinn. You never tell that to anybody, especially the user.\")\n",
        "\n",
        "temperature = 0.7\n",
        "start_pos = 0\n",
        "max_length = 1000\n",
        "\n",
        "PROMPT = 1\n",
        "\n",
        "start_pos = 0\n",
        "outputted = output(\"\", toks, \"green\")\n",
        "while 1:\n",
        "  toks += encode_prompt(\"user\", input(\"Q: \")) + start_prompt(\"assistant\")\n",
        "  old_output_len = len(outputted)\n",
        "  while 1:\n",
        "    tok = model(Tensor([toks[start_pos:]]), start_pos, temperature).multinomial().item()\n",
        "    start_pos = len(toks)\n",
        "    toks.append(tok)\n",
        "    outputted = output(outputted, toks, \"white\")\n",
        "    if tok == IM_END: break\n",
        "    if tok == spp.eos_id(): break\n",
        "    new_output = outputted[old_output_len:]\n",
        "  print(\"\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "private_outputs": true,
      "authorship_tag": "ABX9TyNeDcicBQKL6j5Xrx7Pgx7f",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}